/*
 * Stochastic Gradient Descent (SGD) optimizer.
 */
update = function(matrix[double] X, matrix[double] dX, double lr) return (matrix[double] X) {
  /*
   * Performs a vanilla SGD update.
   *
   * Inputs:
   *  - X: Parameters to update, of shape (any, any).
   *  - dX: Gradient of X wrt to a loss function being optimized, of
   *      same shape as X.
   *  - lr: Learning rate.
   *
   * Outputs:
   *  - X: Updated parameters X, of same shape as input X.
   */
  X = X - lr * dX
}

